[
  {
    "objectID": "nb/lec2.html",
    "href": "nb/lec2.html",
    "title": "Lecture 2 - Multiple Testing",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm"
  },
  {
    "objectID": "nb/lec2.html#introductory-demo-on-probability-density",
    "href": "nb/lec2.html#introductory-demo-on-probability-density",
    "title": "Lecture 2 - Multiple Testing",
    "section": "Introductory demo on probability density",
    "text": "Introductory demo on probability density\n\n\nn = 2000\nX = np.random.normal(0, 1, n)\n\n\nbin_width = 0.05\nbins = np.arange(-3, 3 + bin_width, bin_width)\n\n## plot the histogram\nweights = np.ones(n) / n \nplt.hist(X, bins=bins, color='blue', \n         edgecolor='black', linewidth=1, \n         weights=weights)\n\nplt.yticks(np.arange(0, 0.5, 0.05)) \nplt.xlabel('x')\nplt.ylabel('Proportion')\nplt.show()\n\n\n## plot the discretized density\nweights = weights/bin_width\nplt.hist(X, bins=bins, color='blue', \n         edgecolor='black', linewidth=1, \n         weights=weights)\n\nplt.xlabel('x')\nplt.yticks(np.arange(0, 2, 0.1))\nplt.ylabel('Density')\n\n# overlay beta density\nxs = np.linspace(-3, 3, 1000)  \npdf = norm.pdf(xs, 0, 1)\nplt.plot(xs, pdf, color='red', linewidth=2)\nplt.show()"
  },
  {
    "objectID": "nb/lec2.html#example-2.1-simple-example-of-bayesian-posterior",
    "href": "nb/lec2.html#example-2.1-simple-example-of-bayesian-posterior",
    "title": "Lecture 2 - Multiple Testing",
    "section": "Example 2.1: simple example of Bayesian posterior",
    "text": "Example 2.1: simple example of Bayesian posterior\n\nimport matplotlib.pyplot as plt\n\ndistributions = {\n    \"Prior\": [1/3, 1/3, 1/3],\n    \"Posterior given X = 2\": [1/4, 1/2, 1/4],\n    \"Posterior given X = 1\": [2/3, 1/3, 0]\n}\n\n\nplt.rcParams.update({'font.size': 14})\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nfor i, (title, probs) in enumerate(distributions.items()):\n    axes[i].bar([1, 2, 3], probs, color=['blue', 'orange', 'green'])\n    axes[i].set_title(title)\n    axes[i].set_xticks([1, 2, 3])\n    axes[i].set_ylim(0, 1)\n    axes[i].set_xlabel('Theta')\n    axes[i].set_ylabel('Probability')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "nb/lec2.html#example-2.2-posterior-of-binomial-mean",
    "href": "nb/lec2.html#example-2.2-posterior-of-binomial-mean",
    "title": "Lecture 2 - Multiple Testing",
    "section": "Example 2.2: posterior of binomial mean",
    "text": "Example 2.2: posterior of binomial mean\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\nn = 50\nnum_heads = 2\n\na = 1 + num_heads\nb = 1 + n - num_heads\n\nx = np.linspace(0, 1, 1000)\ny = beta.pdf(x, a, b)\n\nplt.figure(figsize=(4, 3))\nplt.plot(x, y)\nplt.title(f\"Posterior with {n} flips and {num_heads} heads\")\nplt.xlabel('x')\nplt.ylim(0, np.max([y.max() * 1.2, 7]))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAdditional example illustrating the notion of posterior\n\nn_sim = 20000\n\nm = 20\n\nresults = np.zeros((n_sim, 2))\n\nfor i in range(n_sim):\n    theta = np.random.uniform(0, 1)\n    S = np.random.binomial(m, theta)\n\n    results[i, 0] = theta\n    results[i, 1] = S\n\n\n## simulated posterior given S = s\ns = 8 \nconditional_thetas = results[results[:, 1] == s, 0]\nnum_instances = conditional_thetas.shape[0]\n\nprint(f\"num instances with S = {s}:  {num_instances}\")\n\nbin_width = 0.01\nbins = np.arange(0, 1 + bin_width, bin_width)\n\n\n## plot the histogram\nweights = np.ones(num_instances) / num_instances \nplt.hist(conditional_thetas, bins=bins, color='blue', \n         edgecolor='black', linewidth=1, \n         weights=weights)\n\nplt.yticks(np.arange(0, 0.5, 0.05)) \nplt.xlabel('Theta')\nplt.ylabel('Proportion')\nplt.show()\n\n\n## plot the discretized density\nweights = weights/bin_width\nplt.hist(conditional_thetas, bins=bins, color='blue', \n         edgecolor='black', linewidth=1, \n         weights=weights)\n\nplt.yticks(np.arange(0, 6, 1)) \nplt.xlabel('Theta')\nplt.ylabel('Density')\n\n# overlay beta density\nx = np.linspace(0, 1, 1000)  \npdf = beta.pdf(x, a=s+1, b=m-s+1)\nplt.plot(x, pdf, color='red', linewidth=2)\n\nplt.show()\n\n\n\nnum instances with S = 8:  963"
  },
  {
    "objectID": "nb/lec2.html#example-2.3-posterior-simulation-for-the-2020-election",
    "href": "nb/lec2.html#example-2.3-posterior-simulation-for-the-2020-election",
    "title": "Lecture 2 - Multiple Testing",
    "section": "Example 2.3: posterior simulation for the 2020 election",
    "text": "Example 2.3: posterior simulation for the 2020 election\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import beta\n\n# Read the CSV file\npolls = pd.read_csv(\"election_polls.csv\")\n\nn_state = len(polls)\nn_sim = 3000\n\nall_results = np.zeros(n_sim)\n\nbias_adjustment = 0.03\n\nfor cur_sim in range(n_sim):\n    all_thetas = np.zeros(n_state)\n    for j in range(n_state):\n        mu = polls.loc[j, \"Percent Trump\"] / 100 + bias_adjustment\n        n = polls.loc[j, \"Sample Size\"]\n        all_thetas[j] = beta.rvs(mu * n + 1, (1 - mu) * n + 1) \n\n    votes = (all_thetas &gt; 0.5) * polls[\"Electors\"]\n    all_results[cur_sim] = sum(votes)\n\nprint(f\"Proportion of simulations where Trump wins: {np.mean(all_results &gt; 269)}\")\n\nplt.figure(figsize=(4, 3))\nplt.hist(all_results, bins=range(50, 500, 10))\nplt.axvline(270, color='black', linestyle='--')\nplt.xlabel('Electoral votes for Trump')\nplt.show()\n\n\n---------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\nCell In[5], line 6\n      3 from scipy.stats import beta\n      5 # Read the CSV file\n----&gt; 6 polls = pd.read_csv(\"election_polls.csv\")\n      8 n_state = len(polls)\n      9 n_sim = 3000\n\nFile ~/anaconda3/envs/stat486/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n   1013 kwds_defaults = _refine_defaults_read(\n   1014     dialect,\n   1015     delimiter,\n   (...)   1022     dtype_backend=dtype_backend,\n   1023 )\n   1024 kwds.update(kwds_defaults)\n-&gt; 1026 return _read(filepath_or_buffer, kwds)\n\nFile ~/anaconda3/envs/stat486/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620, in _read(filepath_or_buffer, kwds)\n    617 _validate_names(kwds.get(\"names\", None))\n    619 # Create the parser.\n--&gt; 620 parser = TextFileReader(filepath_or_buffer, **kwds)\n    622 if chunksize or iterator:\n    623     return parser\n\nFile ~/anaconda3/envs/stat486/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620, in TextFileReader.__init__(self, f, engine, **kwds)\n   1617     self.options[\"has_index_names\"] = kwds[\"has_index_names\"]\n   1619 self.handles: IOHandles | None = None\n-&gt; 1620 self._engine = self._make_engine(f, self.engine)\n\nFile ~/anaconda3/envs/stat486/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880, in TextFileReader._make_engine(self, f, engine)\n   1878     if \"b\" not in mode:\n   1879         mode += \"b\"\n-&gt; 1880 self.handles = get_handle(\n   1881     f,\n   1882     mode,\n   1883     encoding=self.options.get(\"encoding\", None),\n   1884     compression=self.options.get(\"compression\", None),\n   1885     memory_map=self.options.get(\"memory_map\", False),\n   1886     is_text=is_text,\n   1887     errors=self.options.get(\"encoding_errors\", \"strict\"),\n   1888     storage_options=self.options.get(\"storage_options\", None),\n   1889 )\n   1890 assert self.handles is not None\n   1891 f = self.handles.handle\n\nFile ~/anaconda3/envs/stat486/lib/python3.13/site-packages/pandas/io/common.py:873, in get_handle(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\n    868 elif isinstance(handle, str):\n    869     # Check whether the filename is to be opened in binary mode.\n    870     # Binary mode does not support 'encoding' and 'newline'.\n    871     if ioargs.encoding and \"b\" not in ioargs.mode:\n    872         # Encoding\n--&gt; 873         handle = open(\n    874             handle,\n    875             ioargs.mode,\n    876             encoding=ioargs.encoding,\n    877             errors=errors,\n    878             newline=\"\",\n    879         )\n    880     else:\n    881         # Binary mode\n    882         handle = open(handle, ioargs.mode)\n\nFileNotFoundError: [Errno 2] No such file or directory: 'election_polls.csv'"
  },
  {
    "objectID": "nb/lec2.html#example-2.4-mcmc-for-posterior-from-pairwise-comparison",
    "href": "nb/lec2.html#example-2.4-mcmc-for-posterior-from-pairwise-comparison",
    "title": "Lecture 2 - Multiple Testing",
    "section": "Example 2.4: MCMC for posterior from pairwise comparison",
    "text": "Example 2.4: MCMC for posterior from pairwise comparison\n\nimport pandas as pd\nimport numpy as np\n\n# Load the data\ngames = pd.read_csv(\"nba2024.csv\")\nn_games = len(games)\n\n# Get all team names\nteams = pd.unique(games[['Winner', 'Loser']].values.ravel('K'))\nn_teams = len(teams)\n\n# Initialize ratings and all_ratings dictionaries\nratings = {team: np.random.choice([1, 2, 3]) for team in teams}\nall_ratings = {team: [] for team in teams}\n\n\nn_MCMC = 1100\nn_Burn = 100\nSAprob = 0.7\nSBprob = 0.9\nABprob = 0.7\nsameprob = 0.5\n\n# MCMC Simulation\nfor j in range(n_MCMC):\n\n    for cur_team in teams:\n        weights = np.ones(3)\n        \n        for i in range(n_games):\n            \n            if cur_team not in [games.iloc[i, 0], games.iloc[i, 1]]:\n                continue\n\n            opponent = games.iloc[i, 1] if cur_team == games.iloc[i, 0] else games.iloc[i, 0]\n            opponent_rating = ratings[opponent]\n            \n            ## if cur_team is the winner\n            if cur_team == games.iloc[i, 0]:\n                if opponent_rating == 1:\n                    weights *= [sameprob, 1-SAprob, 1-SBprob]\n                elif opponent_rating == 2:\n                    weights *= [SAprob, sameprob, 1-ABprob]\n                else:\n                    weights *= [SBprob, ABprob, sameprob]\n            else: # if cur_team is the loser\n                if opponent_rating == 1:\n                    weights *= [sameprob, SAprob, SBprob]\n                elif opponent_rating == 2:\n                    weights *= [1-SAprob, sameprob, ABprob]\n                else:\n                    weights *= [1-SBprob, 1-ABprob, sameprob]\n        \n        ratings[cur_team] = np.random.choice([1, 2, 3], p=weights/weights.sum())\n\n    if j &gt; n_Burn:\n        all_ratings[cur_team].append(ratings[cur_team])\n\n\n\n# Printing the results\nfor cur_team in teams:\n\n    cur_posterior = np.zeros(3)\n    for j in range(3):\n        cur_posterior[j] = np.mean(np.array(all_ratings[cur_team]) == j+1)\n\n    ## create small plot\n    plt.figure(figsize=(3, 1))\n    plt.bar([1, 2, 3], cur_posterior, color=['blue', 'orange', 'green'])\n    plt.title(f\"Posterior rating for {cur_team}\")\n    plt.show()"
  },
  {
    "objectID": "nb/getting-started.html",
    "href": "nb/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "We will use Python via the Anaconda distribution.\nDownload Anaconda here.\n\n\n\n\n\n\nNote\n\n\n\nThere are a variety of different Python distributions; for statistics and machine learning, we recommend Anaconda as it comes with many useful packages pre-installed.\n(It is also a good idea NOT to use your computer’s pre-installed Python - you don’t want to accidentally change any system settings!)\n\n\nAnaconda requires a few GBs of storage - a more lightweight version is Miniconda, which you can download here.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "nb/getting-started.html#anaconda-installation",
    "href": "nb/getting-started.html#anaconda-installation",
    "title": "Getting started",
    "section": "",
    "text": "We will use Python via the Anaconda distribution.\nDownload Anaconda here.\n\n\n\n\n\n\nNote\n\n\n\nThere are a variety of different Python distributions; for statistics and machine learning, we recommend Anaconda as it comes with many useful packages pre-installed.\n(It is also a good idea NOT to use your computer’s pre-installed Python - you don’t want to accidentally change any system settings!)\n\n\nAnaconda requires a few GBs of storage - a more lightweight version is Miniconda, which you can download here.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "nb/getting-started.html#vscode",
    "href": "nb/getting-started.html#vscode",
    "title": "Getting started",
    "section": "VSCode",
    "text": "VSCode\nThere are a number of Python IDEs (integrated development environments). In class, we will be using VSCode (download here).",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "nb/getting-started.html#managing-packages",
    "href": "nb/getting-started.html#managing-packages",
    "title": "Getting started",
    "section": "Managing packages",
    "text": "Managing packages\nThere are many open source Python packages for statistics and machine learning.\nTo download packages, two popular package managers are Conda and Pip. Both Conda and Pip come with the Anaconda distribution.\nConda is a general-purpose package management system, designed to build and manage software of any type from any language. This means conda can take advantage of many non-python packages (like BLAS, for linear algebra operations).\nPip is a package manager for python. You may see people using pip with environments using virtualenv or venv.\nWe recommend:\n\nuse a conda environment\nwithin this environment, use conda to install base packages such as pandas and numpy\nif a package is not available via conda, then use pip\n\nSee here for some conda vs pip misconceptions, and why conda is helpful.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "nb/getting-started.html#environments",
    "href": "nb/getting-started.html#environments",
    "title": "Getting started",
    "section": "Environments",
    "text": "Environments\n\nAbout\nIt is good coding practice to use virtual environments with Python. From this blog:\n\nA Python virtual environment consists of two essential components: the Python interpreter that the virtual environment runs on and a folder containing third-party libraries installed in the virtual environment. These virtual environments are isolated from the other virtual environments, which means any changes on dependencies installed in a virtual environment don’t affect the dependencies of the other virtual environments or the system-wide libraries. Thus, we can create multiple virtual environments with different Python versions, plus different libraries or the same libraries in different versions.\n\n\n\n\nCreating an environment for STAT-486\nWe recommend creating a virtual environment for your STAT-486 coding projects. This way, you can have an environment with all the necessary packages and you can easily keep track of what versions of the packages you used.\n\nOpen Terminal (macOS) or a shell. You can also use Terminal in VSCode.\nCreate an environment called stat486 using Conda with the command: conda create --name stat486\nTo install packages in your environment, first activate your environment: conda activate stat486\nThen, install the following packages using the command: conda  install numpy pandas scikit-learn matplotlib seaborn jupyter ipykernel\nInstall PyTorch by running the appropriate command from here (for macOS, the command is: pip3 install torch torchvision)\nTo exit your environment: conda deactivate\n\nHere is a helpful cheatsheet for conda environment commands.\nFor more details about the shell / bash, here is a helpful resource.\n\n\nJupyter Notebooks\n\nDownload lec1.ipynb here and open it in VSCode.\nTo use your stat486 environment, on the top right hand corner, click “Select Kernel” &gt; “Python Environments” &gt; stat486. If it prompts you to install ipykernel, follow the prompts to install it.\n\nJupyter notebooks (.ipynb files) are useful to combine code cells with text (as markdown cells).\nVSCode also has a Python interactive window (details here).",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "nb/getting-started.html#learning-python",
    "href": "nb/getting-started.html#learning-python",
    "title": "Getting started",
    "section": "Learning Python",
    "text": "Learning Python\nHere are some resources for learning Python:\n\nNumpy\nPandas\nObject-oriented programming",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rutgers STAT-486 - Statistical Learning",
    "section": "",
    "text": "Instructor: Gemma Moran\nInstructor website: link"
  },
  {
    "objectID": "index.html#spring-2026",
    "href": "index.html#spring-2026",
    "title": "Rutgers STAT-486 - Statistical Learning",
    "section": "",
    "text": "Instructor: Gemma Moran\nInstructor website: link"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Rutgers STAT-486 - Statistical Learning",
    "section": "Getting started",
    "text": "Getting started\nSee here for details about how to get started with python, VSCode and PyTorch."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Rutgers STAT-486 - Statistical Learning",
    "section": "Lectures",
    "text": "Lectures\nThe code from the lectures is in the left sidebar.\nLecture slides and information about homeworks, office hours etc. can be found on Canvas."
  },
  {
    "objectID": "nb/lec1.html",
    "href": "nb/lec1.html",
    "title": "Lecture 1 - Monte Carlo",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats",
    "crumbs": [
      "Home",
      "Lecture 1 - Monte Carlo"
    ]
  },
  {
    "objectID": "nb/lec1.html#introduction",
    "href": "nb/lec1.html#introduction",
    "title": "Lecture 1 - Monte Carlo",
    "section": "Introduction",
    "text": "Introduction\n\nExample 1.2: simple binomial test\n\nnum_simulation = 1000\nnum_tosses = 6\nobs_num_heads = 1\n\nall_results = np.zeros(num_simulation)\n\nfor cur_sim in range(num_simulation):\n    cur_tosses = np.zeros(num_tosses)\n    for i in range(num_tosses):\n        cur_tosses[i] = np.random.choice([0, 1])\n\n    all_results[cur_sim] = np.sum(cur_tosses)\n\nsim_pval = np.sum(all_results &lt;= obs_num_heads) / num_simulation\n\nprint(f\"Simulated p-value: {sim_pval}\")\n\nSimulated p-value: 0.106\n\n\nPlotting the distribution of the test statistic under the null\n\nbins = np.arange(all_results.min(), all_results.max() + 2) - 0.5\n\nplt.hist(all_results, rwidth=.7, bins=bins);\nplt.axvline(obs_num_heads, color='red');\nplt.show()",
    "crumbs": [
      "Home",
      "Lecture 1 - Monte Carlo"
    ]
  },
  {
    "objectID": "nb/lec1.html#permutation-tests",
    "href": "nb/lec1.html#permutation-tests",
    "title": "Lecture 1 - Monte Carlo",
    "section": "Permutation tests",
    "text": "Permutation tests\n\nExample 1.3: A/B Testing\nAlso known as two sample testing.\nConsider an A/B test where each user outcome is binary (click = 1, no click = 0). We want to test whether the click-through rates differ between the two groups:\nH_0: p_A = p_B \\quad \\text{vs.} \\quad H_1: p_A\\neq p_B\nA permutation test constructs a null distribution by repeatedly shuffling the A/B labels.\nUnder the null hypothesis, the labels do not carry information about CTR; that is, the outcomes are exchangeable across groups. Therefore, conditional on the observed outcomes, every reassignment of the A/B labels (keeping the group sizes fixed) is equally likely.\nTo test H_0:\n\nCompute a test-statistic from observed data (e.g. T^{obs} = |\\widehat{p}_A - \\widehat{p}_B|)\nRandomly shuffle the A/B labels many times and recompute test statistic each time\nCompare T^{obs} to the permutation distribution to obtain a p-value:\n\np\\text{-val} = \\frac{1}{S}\\sum_{s=1}^S \\mathbb{I}(T^{(s)} \\geq T^{obs})\n\nn_sim = 1000\n\nmy_viewsA = 98\nmy_viewsB = 162\nall_views = my_viewsA + my_viewsB\n\nn_impsA = 1000\nn_impsB = 2000\nall_imps = n_impsA + n_impsB\n\nobs_T = abs(my_viewsA / n_impsA - my_viewsB / n_impsB)\n\nnull_Ts = np.zeros(n_sim) ## what we called \"all_results\" from before\n\nfor cur_sim in range(n_sim):\n    pool = np.array([1] * all_views + [0] * (all_imps - all_views))\n    impsA = np.random.choice(pool, n_impsA, replace=False)\n    viewsA = np.sum(impsA)\n    viewsB = all_views - viewsA\n\n    diff = viewsA / n_impsA - viewsB / n_impsB\n    null_Ts[cur_sim] = diff\n\nsim_pval = np.sum(np.abs(null_Ts) &gt;= np.abs(obs_T)) / n_sim\n\nprint(f\"Simulated p-value: {sim_pval}\")\n\nplt.hist(null_Ts, bins=12)\nplt.axvline(abs(obs_T), color='red')\nplt.show()\n\nSimulated p-value: 0.137\n\n\n\n\n\n\n\n\n\n\n\nExample 1.4: independence test for contingency tables\nWe have paired data (X_1,Y_1), \\dots, (X_n, Y_n).\nWe can use permutations to test:\nH_0: X \\text{ independent of } Y \\quad \\text{vs.}\\quad H_1: X \\text{ not independent of } Y\nUnder the null, shuffling which values are paired does not change the distribution (because X and Y are independent).\nThat is, under the null, (X_4, Y_1), (X_{32}, Y_2), ... (X_5, Y_n) has the same distribution as the original data.\n\nK1 = 3\nK2 = 2\n\ncon_table = [[350, 1200, 450], \n             [20, 120, 60]]\n\ncon_table = np.array(con_table)\n\ncolumn_sums = np.sum(con_table, axis=0)\nrow_sums = np.sum(con_table, axis=1)\n\n\n## compute chi-squared test statistic\nE = np.zeros((K2, K1))\nfor i in range(K2):\n    for j in range(K1):\n        E[i, j] = row_sums[i] * column_sums[j] / np.sum(con_table)\n\nobs_T = np.sum((con_table - E)**2 / E)\n\nn = int(np.sum(con_table))\n\n\n## convert contingency table to data pairs\ndata_pairs = []\nfor i in range(K2):\n    for j in range(K1):\n        for _ in range(int(con_table[i, j])):\n            data_pairs.append([i, j])\ndata_pairs = np.array(data_pairs)\n\n\nn_sim = 1000\n\nall_null_Ts = np.zeros(n_sim)\n\nfor cur_sim in range(n_sim):\n    ## permute the first column of data_pairs\n    cur_sim_data = np.column_stack((np.random.choice(data_pairs[:, 0], n, replace=False), data_pairs[:, 1]))\n\n    ## convert cur_sim_data to contingency table\n    cur_sim_con_table = np.zeros((K2, K1))\n    for i in range(n):\n        cur_sim_con_table[cur_sim_data[i, 0], cur_sim_data[i, 1]] += 1\n    \n    null_T = np.sum((cur_sim_con_table - E)**2 / E)\n    all_null_Ts[cur_sim] = null_T\n\nprint(cur_sim_con_table)\n\n## compute p-value\np_value = np.mean(all_null_Ts &gt;= obs_T)\nprint(f\"Simulated p-value using test statistic 1 = {p_value}\")\n\nplt.hist(all_null_Ts, bins=12);\nplt.axvline(obs_T, color='red');\n\n[[ 332. 1205.  463.]\n [  38.  115.   47.]]\nSimulated p-value using test statistic 1 = 0.011\n\n\n\n\n\n\n\n\n\n\n## Use traditional pearson chi-squared test\nfrom scipy.stats import chi2_contingency\n\nchi2, p, dof, expected = chi2_contingency(con_table)\nprint(f\"CLT-based p-value = {p:.4f}\")\n\nCLT-based p-value = 0.0053\n\n\n\n\nExample 1.5: independence test for continuous data\n\n# Creating the X matrix\nX = np.array([[2.5, 2.7], [4, 4.0], [5, 3.2], [1, 2.7], [3, 3.2], [2, 2.4], [1.5, 2.1]])\n\n## plot variable X[, 0] vs X[, 1]\nplt.scatter(X[:, 0], X[:, 1])\nplt.xlabel(\"X1\")\nplt.ylabel(\"X2\")\n\nnp.corrcoef(X[:, 0], X[:, 1])[0, 1]\n\nnp.float64(0.7350457786848433)\n\n\n\n\n\n\n\n\n\n\nn = X.shape[0]\n\n# Calculating the correlation of the original data\nobs_T = np.corrcoef(X[:, 0], X[:, 1])[0, 1]\n\nn_sim = 1000\n\nnull_Ts = np.zeros(n_sim)\n\nfor cur_sim in range(n_sim):\n    # Shuffling only the first column of X\n    Xprime = np.column_stack((np.random.choice(X[:, 0], n, replace=False), X[:, 1]))\n    null_Ts[cur_sim] = np.corrcoef(Xprime[:, 0], Xprime[:, 1])[0, 1]\n\n# Calculating the proportion of simulations where the absolute correlation is greater than my_corr\nsim_pval = np.sum(np.abs(null_Ts) &gt;= obs_T) / n_sim\n\nprint(f\"Simulated p-value: {sim_pval}\")\n\nplt.hist(null_Ts, bins=12)\nplt.axvline(obs_T, color='red')\nplt.show()\n\nSimulated p-value: 0.046\n\n\n\n\n\n\n\n\n\n\n\nExample 1.6: independence test using Spearman’s rho\n\nx = np.arange(1, 20)\ny = np.arange(1, 20)\ny[-1] = -50\n\nn = len(x)\n\nplt.figure(figsize=(5,4))\nplt.scatter(x, y);\nplt.xlabel('x');\nplt.ylabel('y');\n\n\n\n\n\n\n\n\n\n# correlation of original data\nobs_corr = np.corrcoef(x, y)[0,1]\n# spearman rho\nrank_x = stats.rankdata(x)\nrank_y = stats.rankdata(y)\nobs_spearman = np.corrcoef(rank_x, rank_y)[0,1]\n\nn_sim = 1000\n\nnull_corr = np.zeros(n_sim)\nnull_spearman = np.zeros(n_sim)\n\nfor cur_sim in range(n_sim):\n    \n    x_shuffle = np.random.choice(x, n, replace=False) \n    null_corr[cur_sim] = np.corrcoef(x_shuffle, y)[0, 1]\n    rank_x = stats.rankdata(x_shuffle)\n    null_spearman[cur_sim] = np.corrcoef(rank_x, rank_y)[0, 1]\n\n# Calculating the proportion of simulations where the absolute correlation is greater than obs_corr\ncorr_pval = np.sum(np.abs(null_corr) &gt;= obs_corr) / n_sim\nprint(f\"Simulated correlation p-value: {corr_pval}\")\n\nspearman_pval = np.sum(np.abs(null_spearman) &gt;= obs_spearman) / n_sim\nprint(f\"Simulated spearman's p-value: {spearman_pval}\")\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\nax1.hist(null_corr, bins=12)\nax1.axvline(obs_corr, color='red')\nax1.set_title('Independence test with correlation')\n\nax2.hist(null_spearman, bins=12)\nax2.axvline(obs_spearman, color='red')\nax2.set_title('Independence test with Spearmans rho')\n\nplt.show()\n\nSimulated correlation p-value: 1.0\nSimulated spearman's p-value: 0.0",
    "crumbs": [
      "Home",
      "Lecture 1 - Monte Carlo"
    ]
  },
  {
    "objectID": "nb/lec1.html#estimating-sampling-distributions",
    "href": "nb/lec1.html#estimating-sampling-distributions",
    "title": "Lecture 1 - Monte Carlo",
    "section": "Estimating sampling distributions",
    "text": "Estimating sampling distributions\nGoal: Given a sample X_1,\\dots, X_n\\sim p(x|\\theta), what is the standard error for an estimator \\widehat{\\theta} of \\theta?\n\nFor example, the sample mean \\bar{X} is an estimator for the population mean \\mu. The Central Limit Theorem tells us that \\bar{X} \\sim N(\\mu, \\sigma^2/n), where \\sigma^2 is the population variance.\n\nMonte Carlo: use repeated sampling from p(x|\\theta)\n\nExample: obtain sampling distribution of the median of the exponential distribution\n\nDraw S samples, each of size n, from the exponential distribution\nFor each sample, calculate the median \\widehat{\\theta}^{(s)} = \\text{median}(X_1^{(s)}, \\dots, X_n^{(s)})\nNow you have an approximate distribution based on (\\widehat{\\theta}^{(1)},\\dots, \\widehat{\\theta}^{(S)})",
    "crumbs": [
      "Home",
      "Lecture 1 - Monte Carlo"
    ]
  },
  {
    "objectID": "nb/lec1.html#the-bootstrap",
    "href": "nb/lec1.html#the-bootstrap",
    "title": "Lecture 1 - Monte Carlo",
    "section": "The Bootstrap",
    "text": "The Bootstrap\nProblem: If we do not know p(x|\\theta), we cannot repeatedly sample from the population\nThe bootstrap (Efron, 1979) refers to a simulation-based approach to understand the accuracy of statistical estimates.\n\nInstead of sampling from p(x|\\theta), the bootstrap involves sampling from an observed sample x_1,\\dots, x_n, with replacement\nThat is, the bootstrap approximates p(x|\\theta) with the empirical distribution of x_1,\\dots, x_n.\n\n\n\nExample 1.7: confidence interval for the median\nHere is a sample X_1,\\dots, X_n from the exponential distribution.\n\n# draw sample\nnp.random.seed(42)\nn = 30\nx = np.random.exponential(scale=1, size=n)  \n\n# calculate sample estimate\nmed_hat = np.median(x)\n\n# get population distribution for plotting\nx_vals = np.linspace(0, 10, 100)\npdf_vals = scipy.stats.expon.pdf(x_vals)\n\nplt.figure(figsize=(4,3))\nplt.plot(x_vals, pdf_vals, 'r', label='Population');\nplt.hist(x, density=True, label='Sample', bins=20);\nplt.title('Exponential Distribution');\nplt.legend();\n\n\n\n\n\n\n\n\n\nprint(f\"Estimated: {med_hat:.3f}\")\n\nWe use the bootstrap to obtain bootstrap samples, and calculate the median of each of the samples.\n\nn_boot = 1000\n\nboot_med_hats = np.zeros(n_boot)\n\nfor cur_boot in range(n_boot):\n    xboot = np.random.choice(x, n, replace=True)\n    boot_med_hats[cur_boot] = np.median(xboot)\n\n\nplt.figure(figsize=(4,3))\nbins_medians = np.linspace(min(boot_med_hats), max(boot_med_hats), 15)\nplt.hist(boot_med_hats, bins=bins_medians,density=True, color='grey', alpha=0.7);\nplt.title('Bootstrap Sample Medians');\n\n\n\n\n\n\n\n\nWe show two ways to compute a confidence interval for the sample median, \\widehat{m}\n\nNormal approximation using standard deviation of bootstrap medians\n\n [\\widehat{m} - z_{1-\\alpha/2} sd(\\widehat{m}), \\widehat{m} + z_{1-\\alpha/2} sd(\\widehat{m})]\nwhere z_{1-\\alpha/2} is the 1-\\alpha/2 quantile of the Gaussian distribution.\n\nz_alpha = stats.norm.ppf(0.975)  # z_{1-alpha/2} for alpha=0.05\nlower_quantile = med_hat - z_alpha * np.std(boot_med_hats)\nupper_quantile = med_hat + z_alpha * np.std(boot_med_hats)\n\n\nprint(f\"Bootstrap normal confidence interval: ({lower_quantile:.3f}, {upper_quantile:.3f})\")\n\nBootstrap normal confidence interval: (0.198, 0.837)\n\n\n\n\n\n\n\n\n\n\n\n\nQuantiles of bootstrap medians\n\n [\\widehat{Q}_{\\alpha/2}, \\widehat{Q}_{1-\\alpha/2}]\nwhere \\widehat{Q}_{\\alpha/2} is the \\alpha/2 sample quantile of the bootstrap medians.\n\nq025 = np.quantile(boot_med_hats, 0.025)\nq975 = np.quantile(boot_med_hats, 0.975)\nprint(f\"Bootstrap quantile confidence interval: ({q025:.3f}, {q975:.3f})\")\n\nBootstrap quantile confidence interval: (0.291, 0.905)\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 1.8: verifying the validity of bootstrap confidence intervals\nWe show the bootstrap confidence intervals have almost (1-\\alpha)/% coverage.\n\nWe draw a sample from the Poisson(\\lambda) distribution with \\lambda=1. The population mean is 1.\nWe draw bootstrap samples and calculate a confidence interval for the mean using the normal approximation\nWe then record how many intervals contain the population mean\n\n\nn_sim = 500\nsuccesses = np.zeros(n_sim)\nz_alpha = stats.norm.ppf(0.975)  # z_{1-alpha/2} for alpha=0.05\nlower_bounds = []\nupper_bounds = []\n\nfor cur_sim in range(n_sim):\n    n = 50\n    x = np.random.poisson(lam=1, size=n)  # Poisson distribution with lambda=1\n    mu = 1\n\n    muhat = np.mean(x)\n\n    n_boot = 500\n    boot_muhats = np.zeros(n_boot)\n    \n    for cur_boot in range(n_boot):\n        xboot = np.random.choice(x, n, replace=True)\n        boot_muhats[cur_boot] = np.mean(xboot)\n\n    boot_std = np.std(boot_muhats) \n    boot_ci = [muhat - z_alpha * boot_std, muhat + z_alpha * boot_std]\n    lower_bounds.append(muhat - z_alpha * boot_std)\n    upper_bounds.append(muhat + z_alpha * boot_std)\n\n    if boot_ci[0] &lt;= mu &lt;= boot_ci[1]:\n        successes[cur_sim] = 1\n\npercent_success = np.sum(successes) / n_sim\n\nprint(f\"Percent of experiments where the confidence interval contains the true mean: {percent_success}\")\n\n# Plot the confidence intervals\nfig, ax = plt.subplots(figsize=(10, 6))\nfor i in range(n_sim):\n    color = 'blue' if successes[i] else 'red'\n    ax.hlines(i, lower_bounds[i], upper_bounds[i], colors=color, linewidth=0.5)\n\nax.axvline(mu, color='black', linestyle='--', label='True mean')\nax.set_xlabel('Mean value')\nax.set_ylabel('Simulation index')\nax.set_title('Bootstrap Confidence Intervals')\nax.legend()\nplt.show()\n\nPercent of experiments where the confidence interval contains the true mean: 0.92\n\n\n\n\n\n\n\n\n\n\n\nExample 1.9 and 1.10: bootstrap tests for the mean\n\nX = np.array([0.2, -1.9, 1.4, -2.7, -1.7, -1.4, 0.3, 1.2, -1.1, -0.2, -2.1])\nn = len(X)\n\nn_boot = 1000\n\nprint(f\"mean of X: {np.mean(X):.3f}  with n={n}\")\n\nobs_T = abs(np.mean(X))\n\nXc = X - np.mean(X)\n\nboot_Ts = np.zeros(n_boot)\n\nfor cur_boot in range(n_boot):\n  Xboot = np.random.choice(Xc, n, replace=True)\n  boot_Ts[cur_boot] = abs(np.mean(Xboot))\n\nboot_pval = sum(boot_Ts &gt;= obs_T)/n_boot\n\nprint(f\"bootstrap p-value: {boot_pval}\")\n\nplt.hist(boot_Ts, bins=12)\nplt.axvline(np.abs(obs_T), color='red', linestyle='dashed')\n\nplt.show()\n\nmean of X: -0.727  with n=11\nbootstrap p-value: 0.078\n\n\n\n\n\n\n\n\n\n\nX = np.array([-1, 3, 5, 1, 10, 2, 9, 6, 6, 2, 4])\nY = np.array([11, -2, 1, 0, 0, 5, 2])\n\nn = len(X)\nm = len(Y)\n\nn_boot = 1000\n\nXc = X - np.mean(X)\nYc = Y - np.mean(Y)\n\nprint(f\"mean of X: {np.mean(X):.3f},  mean of Y: {np.mean(Y):.3f}\")\n\nobs_T = np.mean(X) - np.mean(Y)\n\nboot_Ts = np.zeros(n_boot)\n\nfor cur_boot in range(n_boot):\n  Xboot = np.random.choice(Xc, n, replace=True)\n  Yboot = np.random.choice(Yc, m, replace=True)\n  boot_Ts[cur_boot] = np.mean(Xboot) - np.mean(Yboot)\n\nboot_pval = sum(np.abs(boot_Ts) &gt;= np.abs(obs_T))/n_boot\n\nprint(f\"bootstrap p-value: {boot_pval}\")\n\nplt.hist(boot_Ts, bins=12)\nplt.axvline(np.abs(obs_T), linestyle='dashed', color='red')\nplt.axvline(-np.abs(obs_T), linestyle='dashed', color='red')\n\nplt.show()\n\nmean of X: 4.273,  mean of Y: 2.429\nbootstrap p-value: 0.324",
    "crumbs": [
      "Home",
      "Lecture 1 - Monte Carlo"
    ]
  }
]